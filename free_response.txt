Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it.  As part of your answer, give some background on the dataset and how it can be used to answer the project question.  Were there any outliers in the data when you got it, and how did you handle those?  [relevant rubric items: “data exploration”, “outlier investigation”]

	The objective of this project is to develop an algorithm to identify people who were "persons of interest" in the enron fraud investigation from two datasets.
	The first dataset contains numerical features obtained from the record of the investigation on findlaw.com, as well as the enron corpus, mentioned below. In this dataset, a person is considered to be a "poi" if they are convicted, fined or reached a settlement in the enron investigation. This information is obtained from a 2005 USA Today article. A few people who should be pois are missed here, namely Lou Pai, who had reached a settlement in 2008, John Baxter, who had committed suicide shortly after testifying, and Andrew Fastow, who agreed to testify in exchange for immunity, but is well known to have been deeply involved in financial fraud. I changed these individuals to pois in the dataset. With the inclusion of these three individuals, 21 of the individuals in this dataset were pois, and 121 were non-poi (so 14.6% pois).
	There were also a number of values that were missing. In the case of this dataset, the lack of a feature is likely useful information. As such, each missing value was replaced with 0.
	Finally, there were a couple entries in the dataset that did not belong, namely "total" and "the travel agency in the park". These were removed.

	The second dataset is the enron corpus, which is a publicly available and widely studied dataset containing emails from individuals within enron. 
	The email addresses in this dataset accounted for only 86 of the individuals in the first dataset (14 pois and 72 non-poi, or 16.3%).
	From this dataset, additional features were extracted representing the relative frequency of the most common words. 

	Machine learning algorithms are particularly useful for this task because they have the capability of recognizing patterns and relationships with a higher level of dimensionality and/or complexity than traditional statistical methods. In this task, we are more concerned with patterns and combinations of features than the overall trends. 



What features did you end up using in your POI identifier, and what selection process did you use to pick them?  Did you have to do any scaling?  Why or why not?  As part of the assignment, you should attempt to engineer your own feature that doesn’t come ready-made in the dataset--explain what feature you tried to make, and the rationale behind it.  (You do not necessarily have to use it in the final analysis, only engineer and test it.)  If you used an algorithm like a decision tree, please also give the feature importances of the features that you use.  [relevant rubric items: “create new features”, “properly scale features”, “intelligently select feature”]

	The following process was used to determine the proper features to use for the numerical dataset.	
	-->conducted a two tailed Mann Whitney U p test on each of the features to determine weather there was a significant difference in the distribution of each feature between the pois and non pois. 
	-->printed out the number of features that were available (not na) for the set of pois and not pois, to determine weather enough features were available to convey useful information.
	-->printed a histogram to get a better sense of the distributions and determine weather or not the results of the mann whitney u test made sense.
	-->looked up the definition of each of the features of interest to verify weather the observed patterns made sense.
	-->Iteratively conducted a PCA decomposition, reducing the feature space each time until each component represented a unique feature.
	-->Reduced three poi email features ("from this person to poi","from_poi to this person" and "shared receipt with poi") to a single principal component (namely "email_feature"), using pca (the addition of this feature significantly improved performance on all classifiers).
	This process yielded the following features to represent the numerical dataset:
		-"deferred_income", 
		-"exercised_stock_options", 
		-"expenses", 
		-"bonus", 
		-"restricted_stock",
		-"email_feature" (see above)

	The following process was used to extract and create features from the enron corpus:
	(see text_operations/get_email_text.ipynb for original feature extraction code.)
	-->extracted all words in the "from_emails" folder of each individual in the dataset (in order to get as close as possible of a representation of the individual's vocabulary).
	-->assembled collective words into array, ran through a TFIDF vectorizer, and extracted the resulting feature values and names.	
	-->removed all words used by fewer than 10 individuals.
	(Note that previous observation revealed that none of the selected words were used exclusively by pois.)
	-->Normalized word features using min_max scaler.
	-->Iteratively fit word values using SelectKBest with f_classif, removing all words representing name's, email addresses, or words with unclear significance.
	-->Created a pca decomposition of the word values resulting from the previous step.
	-->Normalized Pca components using min_max scaler.
	-->Fit the resulting pca features using SelectKBest with f_classif.
	-->Tuned the number of word and pca features obtained from SelectKBest to the optimal values.
	This process yielded 45 features. Ultimately, this feature set (the pca features obtained from the email data) was used in the classification, since these features alone performed much better on the accuracy, precision and recall than any other combination of features and classifier that was tried.



What algorithm did you end up using?  What other one(s) did you try? [relevant rubric item: “pick an algorithm”]

	A gaussian naive bayes classifier was used on the pca features obtained from the email dataset. I also tried (iteratively) a dicision tree classifier, a k nearest neighbors classifier, a random forest classifier, an adaboost classifier, and a gaussian naive bayes classifier on various combinations of the data.
	None of the classifiers used with the numerical dataset performed as well as the Gaussian Naive Bayes algorithm on the email dataset in accuracy and recall. K nearest neighbors, using the numerical dataset, was capable of acheiving perfect recall, but with a very low precision.
	In addition to that, I created and tested my own classifier that combined the results of Gaussian naive bayes on the email dataset with the results of k nearest neighbors on the numerical dataset. This however resulted in a lower preformance.


What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm?  (Some algorithms don’t have parameters that you need to tune--if this is the case for the one you picked, identify and briefly explain how you would have done it if you used, say, a decision tree classifier). [relevant rubric item: “tune the algorithm”]

	Tuning the parameters of an algorithm entails iteratively changing the rules of an algorithm through trial and error to obtain the best results. Not doing this can result in a performance of the algorithm that is significantly lower than it should be. Gaussian Naive Bayes does not have any parameters, however, the number of words and the number of pca components used to generate the input features were carefully tuned to yield the best result.
	For the K-Neighbors algorithm, using the financial dataset, the objective was to obtain a classifier with near perfect precision (so as to improve the results of the previous classifier). The most influential parameters were "n_neighbors", the number of neighbors used in the classifier, and "weights", the method for weighting the neighbors selected. Changing the weights parameter to distance improved the precision of the classifier. Then, increasing the number of neighbors to 10 improved the precision more, though it caused the recall to drop. The perfect balance was at 15 neighbors. This allowed the algorithm to detect a only a small percentage of the poi's, but do so almost no false positives.



What is validation, and what’s a classic mistake you can make if you do it wrong?  How did you validate your analysis?  [relevant rubric item: “validation strategy”]

	Validation is the process of evaluating the ability of a classifier to generalize to a larger population, by testing it on a dataset that is separate, but similar to the one it was trained on. Improper validation may allow for an overfit classifier to pass as a highly accurate solution.
	Since this particular dataset and the distribution of pois was vary small, stratified shuffle split was used to create multiple different randomized training and testing sets from the same dataset while preserving the original distribution of pois. The classifier was then iteratively trained and tested on each of the sets, and the results were aggregated.

Give at least 2 evaluation metrics, and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance. [relevant rubric item: “usage of evaluation metrics”]

	Accuracy: 0.98024
	Accuracy is the percentage of correct predictions. This accuracy is really good, though accuracy alone can be a bit misleading if the distribution of labels is uneven, as is the case with this dataset.

	Precision: 0.89188
	Precision determines how good an algorithm is at distinguishing between a particular class (pois in this case) and all other classes when predicting that class. This value means that about 90% of the predictions of poi that the algorithm made were correct!

	Recall: 0.93675
	Recall determines how good an algorithm is at finding a particular class (pois in this case). This value means that the algorithm was able to find about 90% of the pois!


