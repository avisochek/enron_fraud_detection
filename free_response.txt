Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it.  As part of your answer, give some background on the dataset and how it can be used to answer the project question.  Were there any outliers in the data when you got it, and how did you handle those?  [relevant rubric items: “data exploration”, “outlier investigation”]

	The objective of this project is to develop an algorithm to identify people who were "persons of interest" in the enron fraud investigation from two datasets.
	The first dataset contains numerical features obtained from the record of the investigation on findlaw.com, as well as the enron corpus, mentioned below. In this dataset, a person is considered to be a "poi" if they are convicted, fined or reached a settlement in the enron investigation. This information is obtained from a 2005 USA Today article. A few people who should be pois are missed here, namely Lou Pai, who had reached a settlement in 2008, John Baxter, who had committed suicide shortly after testifying, and Andrew Fastow, who agreed to testify in exchange for immunity, but is well known to have been deeply involved in financial fraud. I changed these individuals to pois in the dataset. With the inclusion of these three individuals, 21 of the individuals in this dataset were pois, and 121 were non-poi (so 14.6% pois).
	There were a number of exceptionally high values in each category. These values did not appear to be erroneous, but rather represent the high level executives, most of whom were not pois. Because there is little data, I replaced the top ten percent of values from each numerical feature with the mean of the remaining values, rather than removing the data point altogether.
	There were also a number of values that were missing. In the case of this dataset, the lack of a feature is likely useful information. As such, each missing value was replaced with 0.
	Finally, there were a couple entries in the dataset that did not belong, namely "total" and "the travel agency in the park". These were removed.

	The second dataset is the enron corpus, which is a publicly available and widely studied dataset containing emails from individuals within enron. 
	The email addresses in this dataset accounted for only 86 of the individuals in the first dataset (14 pois and 72 non-poi, or 16.3%).
	From this dataset, additional features were extracted representing the relative frequency of the most common words. 

	Machine learning algorithms are particularly useful for this task because they have the capability of recognizing patterns and relationships with a higher level of dimensionality and/or complexity than traditional statistical methods. In this task, we are more concerned with patterns and combinations of features than the overall trends. 



What features did you end up using in your POI identifier, and what selection process did you use to pick them?  Did you have to do any scaling?  Why or why not?  As part of the assignment, you should attempt to engineer your own feature that doesn’t come ready-made in the dataset--explain what feature you tried to make, and the rationale behind it.  (You do not necessarily have to use it in the final analysis, only engineer and test it.)  If you used an algorithm like a decision tree, please also give the feature importances of the features that you use.  [relevant rubric items: “create new features”, “properly scale features”, “intelligently select feature”]

	The following process was used to determine the proper features to use for the numerical dataset.	
	-->conducted a two tailed Mann Whitney U p test on each of the features to determine weather there was a significant difference in the distribution of each feature between the pois and non pois. 
	-->printed out the number of features that were available (not na) for the set of pois and not pois, to determine weather enough features were available to convey useful information.
	-->printed a histogram to get a better sense of the distributions and determine weather or not the results of the mann whitney u test made sense.
	-->looked up the definition of each of the features of interest to verify weather the observed patterns made sense.
	-->Iteratively conducted a PCA decomposition, reducing the feature space each time until each component represented a unique feature.
	-->Reduced three poi email features ("from this person to poi","from_poi to this person" and "shared receipt with poi") to a single principal component (namely "email_feature"), using pca.
	This process yielded the following features to represent the numerical dataset:
		-"deferred_income", 
		-"exercised_stock_options", 
		-"expenses", 
		-"bonus", 
		-"restricted_stock",
		-"email_feature" (see above)

	The following process was used to extract and create features from the enron corpus:
	(see text_operations/get_email_text.ipynb for original feature extraction code.)
	-->extracted all words in the "from_emails" folder of each individual in the dataset (in order to get as close as possible of a representation of the individual's vocabulary).
	-->assembled collective words into array, ran through a TFIDF vectorizer, and extracted the resulting feature values and names.	
	-->removed all words used by fewer than 10 individuals, and greater than 30 individuals.
	-->Normalized, and iteratively fit word values using SelectKBest with f_classif, removing all words representing name's, email addresses, or words with unclear significance.
	-->Created a pca decomposition of the word values resulting from the previous step.
	-->Fit the resulting pca features using SelectKBest with f_classif.
	-->Tuned the number of word and pca features obtained from SelectKBest to the optimal values.
	This process yielded 20 features used in the classification. Ultimately, this feature set (obtained from the email data) was used in the classification.



What algorithm did you end up using?  What other one(s) did you try? [relevant rubric item: “pick an algorithm”]

	A gaussian naive bayes classifier was used on the email dataset. I also tried a dicision tree classifier, a k nearest neighbors classifier, a random forest classifier, an adaboost classifier, and a gaussian naive bayes classifier on the numerical dataset.
	In addition to that, I created and tested my own classifier that combined the results of Gaussian naive bayes on the email dataset with the results of k nearest neighbors on the numerical dataset.



What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm?  (Some algorithms don’t have parameters that you need to tune--if this is the case for the one you picked, identify and briefly explain how you would have done it if you used, say, a decision tree classifier). [relevant rubric item: “tune the algorithm”]

	Tuning the parameters of an algorithm entails iteratively changing the rules of an algorithm through trial and error to obtain the best results. Not doing this can result in a performance of the algorithm that is significantly lower than it should be. Gaussian Naive Bayes does not have any parameters, however, the number of words and the number of pca components used to generate the input features were carefully tuned to yield the best result.
	For the K-Neighbors algorithm, using the financial dataset, the objective was to obtain a classifier with near perfect precision (so as to improve the results of the previous classifier). The most influential parameters were "n_neighbors", the number of neighbors used in the classifier, and "weights", the method for weighting the neighbors selected. Changing the weights parameter to distance improved the precision of the classifier. Then, increasing the number of neighbors to 10 improved the precision more, though it caused the recall to drop. The perfect balance was at 15 neighbors. This allowed the algorithm to detect a only a small percentage of the poi's, but do so almost no false positives.



What is validation, and what’s a classic mistake you can make if you do it wrong?  How did you validate your analysis?  [relevant rubric item: “validation strategy”]

	Validation is the process of evaluating the ability of a classifier to generalize to a larger population, by testing it on a dataset that is separate, but similar to the one it was trained on. Improper validation may allow for an overfit classifier to pass as a highly accurate solution.
	Since this particular dataset was so small, k-fold cross validation was used in order to iteratively train and test the classifier on several different training and testing sets from the same dataset.


Give at least 2 evaluation metrics, and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance. [relevant rubric item: “usage of evaluation metrics”]
	Accuracy: 0.98024
	Accuracy is the percentage of correct predictions. Accuracy alone can be a bit misleading however if the distribution of labels is uneven, as is the case with this dataset.
	Precision: 0.89188	
	In this case, the precision refers to 

 Also, because there were many more non-poi's than there were poi's, and because we are concerned mainly with detecting fraud, precision and recall were taken into account along with the measure of accuracy in determining the goodness of the fit.t
